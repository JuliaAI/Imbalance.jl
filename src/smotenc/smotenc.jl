# functions to extract the continuous and categorical parts of a vector or matrix
get_cont_part(x::AbstractVector, split_ind::Int) = @view x[1:split_ind-1]
get_cat_part(x::AbstractVector, split_ind::Int) = @view x[split_ind:end]
get_cont_part(X::AbstractMatrix, split_ind::Int) = @view X[1:split_ind-1, :]
get_cat_part(X::AbstractMatrix, split_ind::Int) = @view X[split_ind:end, :]


# SMOTE-NC uses KNN with a modified distance metric
struct EuclideanWithPenalty <: Metric 
    split_ind::Int
    penalty::Float64
end

"""
Given a matrix of observations `X`, find the median of the standard deviations of the
continuous features of the observations and return that as the penalty.

# Arguments
- `X::AbstractMatrix`: A matrix where each row is an observation
- `split_ind::Int`: The index of the first categorical variable

# Returns
- `Float64`: The penalty term that modifies the distance metric

"""
function get_penalty(
    X::AbstractMatrix{<:AbstractFloat},
    split_ind::Int
)
    Xcont = get_cont_part(X, split_ind)
    σs = vec(std(Xcont, dims = 2))
    σₘ  = median(σs)
    return σₘ
end


"""
This overloads the `evaluate` function of the `Metric` struct to use the modified
distance metric which adds a penalty for each pair of corresponding categorical
variables that are not equal.

# Arguments
- `d::EuclideanWithPenalty`: The modified distance metric
- `x₁::AbstractVector`: First observation
- `x₂::AbstractVector`: Second observation

# Returns
- `Float64`: The distance between `x₁` and `x₂` using the modified distance metric
"""
function Distances.evaluate(d::EuclideanWithPenalty, x₁, x₂)
    x₁_cont = get_cont_part(x₁, d.split_ind)
    x₂_cont = get_cont_part(x₂, d.split_ind)
    e = euclidean(x₁_cont, x₂_cont)
    x₁_cat = get_cat_part(x₁, d.split_ind)
    x₂_cat = get_cat_part(x₂, d.split_ind)
    h = hamming(x₁_cat, x₂_cat)
    return e + d.penalty * h   
end


"""
Choose a random point from the given observations matrix `X` and generate a new point that
in terms of the continuous part, randomly lies in the line joining the random point and 
randomly one of its k-nearest neighbors and in terms of the categorical part, the new point
has the mode of the categorical part of the k-nearest neighbors of the random point.

# Arguments
- `X::AbstractMatrix`: A matrix where each row is an observation
- `tree`: A k-d tree representation of the observations matrix X
- `split_ind::Int`: The index of the first categorical variable
- `k::Int`: Number of nearest neighbors to consider
- `rng::AbstractRNG`: Random number generator

# Returns
- `AbstractVector`: A new observation generated by SMOTE
"""
function generate_new_smotenc_point(
    X::AbstractMatrix{<:AbstractFloat},
    tree,
    split_ind::Int;
    k::Int,
    rng::AbstractRNG,
)
    x_rand = randcols(rng, X)
    x_randneigh, Xneighs = get_random_neighbor(X, tree, x_rand; k, rng, return_all = true)
    # find the continuous part
    x_rand_cont = get_cont_part(x_rand, split_ind)
    x_randneigh_cont = get_cont_part(x_randneigh, split_ind)
    x_new_cont = get_collinear_point(x_rand_cont, x_randneigh_cont; rng = rng)
    # find the categorical part
    Xneighs_cat = get_cat_part(Xneighs, split_ind)
    x_new_cat= get_neighbors_mode(Xneighs_cat, rng)
    return vcat(x_new_cont, x_new_cat)
end




"""
Assuming that all the observations in the observation matrix X belong to the same class,
use SMOTE-NC to generate `n` new observations for that class.

# Arguments
- `X::AbstractMatrix`: A matrix where each row is an observation
- `n::Int`: Number of new observations to generate
- `k::Int`: Number of nearest neighbors to consider.
- `split_ind::Int`: The index of the first categorical variable
- `rng::AbstractRNG`: Random number generator

# Returns
- `AbstractMatrix`: A matrix where each row is a new observation generated by SMOTE
"""
function smotenc_per_class(
    X::AbstractMatrix{<:AbstractFloat},
    n::Int,
    split_ind::Int;
    k::Int = 5,
    rng::AbstractRNG = default_rng(),
)
    size(X, 2) == 1 && (warn("class with a single observation will be ignored"); return X)
    k = (k > 0) ? min(k, size(X, 1) - 1) : 1
    σₘ  = get_penalty(X, split_ind)
    metric = EuclideanWithPenalty(split_ind, σₘ)
    tree = BallTree(X, metric)
    return hcat([generate_new_smotenc_point(X, tree, split_ind; k, rng) for i = 1:n]...)
end


"""
    function smotenc(
        X, y::AbstractVector, split_ind::Int;
        k::Int=5, ratios=nothing, rng::Union{AbstractRNG, Integer}=default_rng()
    )

Oversample a dataset given by a matrix or table of observations `X` and an abstract vector of labels y using SMOTE.

$DOC_MAIN_ARGUMENTS
- `split_ind::Int`: The index of the first categorical variable
- `k::Int`: Number of nearest neighbors to consider in the SMOTE algorithm. 
    Should be within the range `[1, size(X, 1) - 1]` else set to the nearest of these two values.
$DOC_RATIOS_ARGUMENT
$DOC_RNG_ARGUMENT
$DOC_RETURNS
"""
function smotenc(
    X::AbstractMatrix{<:AbstractFloat},
    y::AbstractVector,
    split_ind::Int;
    k::Int = 5,
    ratios = nothing,
    rng::Union{AbstractRNG,Integer} = default_rng(),
)
    rng = rng_handler(rng)
    Xover, yover = generic_oversample(X, y, smotenc_per_class, split_ind::Int; ratios, k, rng)
    return Xover, yover
end


### To be revisited!
function smotenc(
    X,
    y::AbstractVector;
    k::Int = 5,
    ratios = nothing,
    rng::Union{AbstractRNG,Integer} = default_rng(),
)
    Xover, yover = tablify(smote, X, y; k, ratios, rng)
    return Xover, yover
end