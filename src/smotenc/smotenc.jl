# functions to extract the continuous and categorical parts of a vector or matrix
get_cont_part(x::AbstractVector, split_ind::Int) = @view x[1:split_ind-1]
get_cat_part(x::AbstractVector, split_ind::Int) = @view x[split_ind:end]
get_cont_part(X::AbstractMatrix, split_ind::Int) = @view X[1:split_ind-1, :]
get_cat_part(X::AbstractMatrix, split_ind::Int) = @view X[split_ind:end, :]


# SMOTE-NC uses KNN with a modified distance metric
struct EuclideanWithPenalty <: Metric
    split_ind::Int
    penalty::Float64
end

"""
Given a matrix of observations `X`, find the median of the standard deviations of the
continuous features of the observations and return that as the penalty.

# Arguments
- `X::AbstractMatrix`: A matrix where each row is an observation
- `split_ind::Int`: The index of the first categorical variable

# Returns
- `Float64`: The penalty term that modifies the distance metric

"""
function get_penalty(X::AbstractMatrix{<:AbstractFloat}, split_ind::Int)
    Xcont = get_cont_part(X, split_ind)
    σs = vec(std(Xcont, dims = 2))
    σₘ = median(σs)
    return σₘ
end


"""
This overloads the `evaluate` function of the `Metric` struct to use the modified
distance metric which adds a penalty for each pair of corresponding categorical
variables that are not equal.

# Arguments
- `d::EuclideanWithPenalty`: The modified distance metric
- `x₁::AbstractVector`: First observation
- `x₂::AbstractVector`: Second observation

# Returns
- `Float64`: The distance between `x₁` and `x₂` using the modified distance metric
"""
function Distances.evaluate(d::EuclideanWithPenalty, x₁, x₂)
    x₁_cont = get_cont_part(x₁, d.split_ind)
    x₂_cont = get_cont_part(x₂, d.split_ind)
    e = euclidean(x₁_cont, x₂_cont)
    x₁_cat = get_cat_part(x₁, d.split_ind)
    x₂_cat = get_cat_part(x₂, d.split_ind)
    h = hamming(x₁_cat, x₂_cat)
    return e + d.penalty * h
end


"""
Choose a random point from the given observations matrix `X` and generate a new point that
in terms of the continuous part, randomly lies in the line joining the random point and 
randomly one of its k-nearest neighbors and in terms of the categorical part, the new point
has the mode of the categorical part of the k-nearest neighbors of the random point.

# Arguments
- `X::AbstractMatrix`: A matrix where each row is an observation
- `tree`: A k-d tree representation of the observations matrix X
- `split_ind::Int`: The index of the first categorical variable
- `k::Int`: Number of nearest neighbors to consider
- `rng::AbstractRNG`: Random number generator

# Returns
- `AbstractVector`: A new observation generated by SMOTE
"""
function generate_new_smotenc_point(
    X::AbstractMatrix{<:AbstractFloat},
    tree,
    split_ind::Int;
    k::Int,
    rng::AbstractRNG,
)
    x_rand = randcols(rng, X)
    x_randneigh, Xneighs = get_random_neighbor(X, tree, x_rand; k, rng, return_all = true)
    # find the continuous part
    x_rand_cont = get_cont_part(x_rand, split_ind)
    x_randneigh_cont = get_cont_part(x_randneigh, split_ind)
    x_new_cont = get_collinear_point(x_rand_cont, x_randneigh_cont; rng = rng)
    # find the categorical part
    Xneighs_cat = get_cat_part(Xneighs, split_ind)
    x_new_cat = get_neighbors_mode(Xneighs_cat, rng)
    return vcat(x_new_cont, x_new_cat)
end




"""
Assuming that all the observations in the observation matrix X belong to the same class,
use SMOTE-NC to generate `n` new observations for that class.

# Arguments
- `X::AbstractMatrix`: A matrix where each row is an observation
- `n::Int`: Number of new observations to generate
- `k::Int`: Number of nearest neighbors to consider.
- `split_ind::Int`: The index of the first categorical variable
- `rng::AbstractRNG`: Random number generator

# Returns
- `AbstractMatrix`: A matrix where each row is a new observation generated by SMOTE
"""
function smotenc_per_class(
    X::AbstractMatrix{<:AbstractFloat},
    n::Int,
    split_ind::Int;
    k::Int = 5,
    rng::AbstractRNG = default_rng(),
)
    size(X, 2) == 1 && (warn("class with a single observation will be ignored"); return X)
    k = (k > 0) ? min(k, size(X, 1) - 1) : 1
    σₘ = get_penalty(X, split_ind)
    metric = EuclideanWithPenalty(split_ind, σₘ)
    tree = BallTree(X, metric)                      # May need to become BruteTree
    return hcat([generate_new_smotenc_point(X, tree, split_ind; k, rng) for i = 1:n]...)
end


"""
    function smotenc(
        X, y::AbstractVector, split_ind::Int;
        k::Int=5, ratios=nothing, rng::Union{AbstractRNG, Integer}=default_rng()
    )

Oversample a dataset given by a matrix or table of observations `X` and an abstract vector of labels y using SMOTE.

# Arguments

$DOC_COMMON_INPUTS

- `split_ind::Int`: The index of the first categorical variable. Only provided if `X` is a matrix with
    label-encoded categorical columns.

- `k::Int`: Number of nearest neighbors to consider in the SMOTE algorithm. 
    Should be within the range `[1, size(X, 1) - 1]` else set to the nearest of these two values.

$DOC_RATIOS_ARGUMENT

$DOC_RNG_ARGUMENT

# Returns

$DOC_COMMON_OUTPUTS
"""
function smotenc(
    X::AbstractMatrix{<:AbstractFloat},
    y::AbstractVector,
    split_ind::Int;
    k::Int = 5,
    ratios = nothing,
    rng::Union{AbstractRNG,Integer} = default_rng(),
)
    rng = rng_handler(rng)
    Xover, yover =
        generic_oversample(X, y, smotenc_per_class, split_ind::Int; ratios, k, rng)
    return Xover, yover
end



function smotenc(
    X,
    y::AbstractVector;
    k::Int = 5,
    ratios = nothing,
    rng::Union{AbstractRNG,Integer} = default_rng(),
)
    Xover, yover = tablify_nc(smotenc, X, y; k, ratios, rng)
    return Xover, yover
end

# A custom wrapper for SMOTE-NC
function tablify_nc(
    matrix_func::Function,
    X,
    y::AbstractVector;
    materialize::Bool = true,
    kwargs...,
)
    # find the categorical and continuous columns
    types = ScientificTypes.schema(X).scitypes
    cat_inds = findall( x -> x <: Multiclass, types)
    cont_inds = findall( x -> x <: Union{Infinite, OrderedFactor}, types)      # add ordered factor (have to document)
    ### Assertion that these are only the types in the input

    # setup the decode transform for categotical columns
    encode_dict = Dict{Int, Function}()
    decode_dict = Dict{Int, Function}()

    columns = Tables.columns(X)
    for c in cat_inds
        column = collect(Tables.getcolumn(columns, c))
        decode_dict[c] = x -> CategoricalDistributions.decoder(column)(round(Int, x))
        encode_dict[c] = x -> CategoricalDistributions.int(x)
    end
    # avoid converting floats to integers
    
    # Encode the data
    Xenc = X |> TableOperations.transform(encode_dict) |> Tables.columntable        
    # TODO: Remove Tables.columntable once https://github.com/JuliaData/TableOperations.jl/issues/32 is resolved

    # Matrixify the table
    Xm, names = matrixify(Xenc)             # Matrix floats       

    # reorder columns so that continuous ones appear first
    Xcont = Xm[:, cont_inds]
    Xcat = Xm[:, cat_inds]
    split_ind = size(Xcont, 2) + 1
    Xm = hcat(Xcont, Xcat)


    # construct an inverse mapping for the column reordering
    cols_map = vcat(cont_inds, cat_inds)
    inv_map = [findfirst(cols_map .== i) for i in 1:length(cols_map)]
    #Xm[:, inv_map] => Xm  (invperm Base)

    # apply the algorithm logic on the matrix
    Xover, yover = matrix_func(Xm, y, split_ind; kwargs...)

    # reorder the columns to their initial ordering
    Xover = Xover[:, inv_map]

    # decode the categorical variables back to categories
    Xover = Tables.table(Xover, header = names) 
    Xover = Xover |> TableOperations.transform(decode_dict)

    # also maintain a way to convert it back to a table
    if materialize
        to_table = Tables.materializer(X)
        Xover = to_table(Xover)
    end
    return Xover, yover
end