"""
Assuming that all the observations in the observation matrix X belong to the same class, 
generate n new observations for that class using ROSE.

# Arguments
- `X`: A matrix where each row is an observation of floats
- `n`: Number of new observations to generate
- `s`: A parameter that proportionally controls the bandwidth of the Gaussian kernel
- `rng`: Random number generator

# Returns
- `Xnew`: A matrix where each column is a new observation generated by ROSE
"""
function rose_per_class(
    X::AbstractMatrix{<:AbstractFloat},
    n::Integer;
    s::AbstractFloat = 1.0,
    rng::AbstractRNG = default_rng(),
)
    # sample n cols from X
    Xnew = randcols(rng, X, n)
    # For s == 0 this is just random oversampling
    s ≈ 0.0 && return Xnew
    # compute the standard deviation feature-wise
    σs = vec(std(X, dims = 2))
    # compute h and then H as in the paper
    d = size(X, 1)  
    N = size(X, 2)
    h = (4 / ((d + 2) * N))^(1 / (d + 4))
    # make a diagonal matrix of the result
    H = Diagonal(σs * s * h)

    # generate standard normal samples of same dimension of Xnew
    XSnew = randn(rng, size(Xnew))
    # matrix multiply the diagonal matrix by XSnew
    XSnew = H * XSnew
    # add Xnew and XSnew
    Xnew += XSnew
    # This is equivalent to sampling from a multivariate normal
    # centered at each point in Xnew with covariance matrix H

    # return the result
    return Xnew
end

"""
    rose(
        X, y; 
        s=1.0, ratios=1.0, rng=default_rng(),
        try_preserve_type=true
    )

# Description

Oversamples a dataset using `ROSE` (Random Oversampling Examples) algorithm to 
    correct for class imbalance as presented in [1]


# Positional Arguments


$(COMMON_DOCS["INPUTS"])

# Keyword Arguments

- `s::float=1.0`: A parameter that proportionally controls the bandwidth of the Gaussian kernel

$(COMMON_DOCS["RATIOS"])

$(COMMON_DOCS["RNG"])

$(COMMON_DOCS["TRY_PRESERVE_TYPE"])


# Returns

$(COMMON_DOCS["OUTPUTS"])

# Example

```julia
using Imbalance

# set probability of each class
class_probs = [0.5, 0.2, 0.3]                         
num_rows, num_continuous_feats = 100, 5
# generate a table and categorical vector accordingly
X, y = generate_imbalanced_data(num_rows, num_continuous_feats; 
                                class_probs, rng=42)  

julia> Imbalance.checkbalance(y)
1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) 
2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) 
0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) 

# apply ROSE
Xover, yover = rose(X, y; s=0.3, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)

julia> Imbalance.checkbalance(yover)
2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) 
1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) 
0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) 
```

# MLJ Model Interface

Simply pass the keyword arguments while initiating the `ROSE` model and pass the 
    positional arguments `X, y` to the `transform` method. 

```julia
using MLJ
ROSE = @load ROSE pkg=Imbalance

# Wrap the model in a machine
oversampler = ROSE(s=0.3, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)
mach = machine(oversampler)

# Provide the data to transform (there is nothing to fit)
Xover, yover = transform(mach, X, y)
```
You can read more about this `MLJ` interface [here]().



# TableTransforms Interface

This interface assumes that the input is one table `Xy` and that `y` is one of the columns. Hence, an integer `y_ind`
    must be specified to the constructor to specify which column `y` is followed by other keyword arguments. 
    Only `Xy` is provided while applying the transform.

```julia
using Imbalance
using Imbalance.TableTransforms

# Generate imbalanced data
num_rows = 200
num_features = 5
y_ind = 3
Xy, _ = generate_imbalanced_data(num_rows, num_features; 
                                 class_probs=[0.5, 0.2, 0.3], insert_y=y_ind, rng=42)

# Initiate ROSE model
oversampler = ROSE(y_ind; s=0.3, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)
Xyover = Xy |> oversampler                              
# equivalently if TableTransforms is used
Xyover, cache = TableTransforms.apply(oversampler, Xy)    # equivalently
```
The `reapply(oversampler, Xy, cache)` method from `TableTransforms` simply falls back to `apply(oversample, Xy)` and the `revert(oversampler, Xy, cache)`
reverts the transform by removing the oversampled observations from the table.


# Illustration
A full basic example along with an animation can be found [here](https://githubtocolab.com/JuliaAI/Imbalance.jl/blob/dev/examples/oversample_rose.ipynb). 
    You may find more practical examples in the [walkthrough](https://juliaai.github.io/Imbalance.jl/dev/examples/) 
    section which also explains running code on Google Colab.

# References

[1] G Menardi, N. Torelli, “Training and assessing classification rules with imbalanced data,” 
Data Mining and Knowledge Discovery, 28(1), pp.92-122, 2014.

"""
function rose(
    X::AbstractMatrix{<:AbstractFloat},
    y::AbstractVector;
    s::AbstractFloat = 1.0,
    ratios = 1.0,
    rng::Union{AbstractRNG,Integer} = default_rng(),
    try_preserve_type::Bool = true,
)
    if s < 0.0
        throw(ArgumentError(ERR_NEG_S(s)))
    end
    rng = rng_handler(rng)
    Xover, yover = generic_oversample(X, y, rose_per_class; s, ratios, rng,)
    return Xover, yover
end

# dispatch for table inputs
function rose(
    X,
    y::AbstractVector;
    s::AbstractFloat = 1.0,
    ratios = 1.0,
    rng::Union{AbstractRNG,Integer} = default_rng(),
    try_preserve_type::Bool = true
)
    Xover, yover = tablify(rose, X, y; try_preserve_type=try_preserve_type, s, ratios, rng, 
                           )
    return Xover, yover
end

# dispatch for table inputs where y is one of the columns
function rose(
    Xy,
    y_ind::Integer;
    s::AbstractFloat = 1.0,
    ratios = 1.0,
    rng::Union{AbstractRNG,Integer} = default_rng(),
    try_preserve_type::Bool = true,
)
    return tablify(rose, Xy, y_ind;try_preserve_type=try_preserve_type,  s, ratios, rng)
end
