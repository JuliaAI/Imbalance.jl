{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this installs Julia 1.7\n",
    "%%capture\n",
    "%%shell\n",
    "wget -O - https://raw.githubusercontent.com/JuliaAI/Imbalance.jl/dev/docs/src/examples/colab.sh | bash\n",
    "#This should take around one minute to finish. Once it does, change the runtime to `Julia` by choosing `Runtime` \n",
    "# from the toolbar then `Change runtime type`. You can then delete this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this section of the docs, we will walk you through some examples to demonstrate how you can use `Imbalance.jl` in your machine learning project. Although we focus on examples, you can learn more about how specific algorithms work by reading this series of blogposts on  [Medium](https://medium.com/towards-data-science/class-imbalance-from-random-oversampling-to-rose-517e06d7a9b).\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In further examples, we will assume familiarity with the [CSV](https://csv.juliadata.org/stable/index.html), [DataFrames](https://dataframes.juliadata.org/stable/), [ScientificTypes](https://juliaai.github.io/ScientificTypes.jl/dev/) and [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/) packages, all of which come with excellent documentation. This example is devoted to assuring and enforcing your familiarity with such packages. You can try this all examples in the docs on your browser using [Google Colab](https://githubtocolab.com/JuliaAI/Imbalance.jl/blob/dev/docs/src/examples/walkthrough.ipynb) and you can read more about that in the last section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;\n",
    "Pkg.add([\"Random\", \"CSV\", \"DataFrames\", \"MLJ\", \n",
    "         \"Imbalance\", \"MLJBalancing\", \"ScientificTypes\", \"HTTP\"])\n",
    "\n",
    "using Random\n",
    "using CSV\n",
    "using DataFrames\n",
    "using MLJ\n",
    "using Imbalance\n",
    "using MLJBalancing\n",
    "using ScientificTypes\n",
    "using HTTP: download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "In this example, we will consider the [BMI dataset](https://www.kaggle.com/datasets/yasserh/bmidataset) found on Kaggle where the objective is to predict the BMI index of individuals given their gender, weight and height. \n",
    "\n",
    "`CSV` gives us the ability to easily read the dataset after it's downloaded as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬────────┬────────┬───────┐\n",
      "│\u001b[1m Gender  \u001b[0m│\u001b[1m Height \u001b[0m│\u001b[1m Weight \u001b[0m│\u001b[1m Index \u001b[0m│\n",
      "│\u001b[90m String7 \u001b[0m│\u001b[90m Int64  \u001b[0m│\u001b[90m Int64  \u001b[0m│\u001b[90m Int64 \u001b[0m│\n",
      "│\u001b[90m Textual \u001b[0m│\u001b[90m Count  \u001b[0m│\u001b[90m Count  \u001b[0m│\u001b[90m Count \u001b[0m│\n",
      "├─────────┼────────┼────────┼───────┤\n",
      "│ Male    │ 174    │ 96     │ 4     │\n",
      "│ Male    │ 189    │ 87     │ 2     │\n",
      "│ Female  │ 185    │ 110    │ 4     │\n",
      "│ Female  │ 195    │ 104    │ 3     │\n",
      "│ Male    │ 149    │ 61     │ 3     │\n",
      "└─────────┴────────┴────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "download(\"https://raw.githubusercontent.com/JuliaAI/Imbalance.jl/dev/docs/src/examples/bmi.csv\")\n",
    "df = CSV.read(\"./bmi.csv\", DataFrame)\n",
    "\n",
    "# Display the first 5 rows with DataFrames\n",
    "first(df, 5) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coercing Data\n",
    "Typical models from `MLJ` assume that elements in each column of a table have some *scientific type* as defined by the [ScientificTypes.jl](https://juliaai.github.io/ScientificTypes.jl/dev/) package. Among the many types defined by the package, we are interested in `Multiclass`, `OrderedFactor` which fall under the `Finite` abstract type and `Continuous` and `Count` which fall under the `Infinite` abstract type.\n",
    "\n",
    "One motivation for this package is that it's not generally obvious whether numerical data in an input table is of continuous type or categorical type given that numbers can describe both. Meanwhile, it's problematic if a model treats numerical data as say `Continuous` or `Count` when it's in reality nominal (i.e., `Multiclass`) or ordinal (i.e., `OrderedFactor`).\n",
    "\n",
    "We can use `schema(df)` to see how each features is currently going to be interpreted by the resampling algorithms: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────┬──────────┬─────────┐\n",
       "│\u001b[22m names  \u001b[0m│\u001b[22m scitypes \u001b[0m│\u001b[22m types   \u001b[0m│\n",
       "├────────┼──────────┼─────────┤\n",
       "│ Gender │ Textual  │ String7 │\n",
       "│ Height │ Count    │ Int64   │\n",
       "│ Weight │ Count    │ Int64   │\n",
       "│ Index  │ Count    │ Int64   │\n",
       "└────────┴──────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ScientificTypes.schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change encodings that are leading to incorrect interpretations (true for all variable in this example), we use the coerce method, as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────┬──────────────────┬───────────────────────────────────┐\n",
       "│\u001b[22m names  \u001b[0m│\u001b[22m scitypes         \u001b[0m│\u001b[22m types                             \u001b[0m│\n",
       "├────────┼──────────────────┼───────────────────────────────────┤\n",
       "│ Gender │ Multiclass{2}    │ CategoricalValue{String7, UInt32} │\n",
       "│ Height │ Continuous       │ Float64                           │\n",
       "│ Weight │ Continuous       │ Float64                           │\n",
       "│ Index  │ OrderedFactor{6} │ CategoricalValue{Int64, UInt32}   │\n",
       "└────────┴──────────────────┴───────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = coerce(df,\n",
    "            :Gender => Multiclass,\n",
    "            :Height => Continuous,\n",
    "            :Weight => Continuous,\n",
    "            :Index => OrderedFactor)\n",
    "ScientificTypes.schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking and Splitting Data\n",
    "\n",
    "Both `MLJ` and the pure functional interface of `Imbalance` assume that the observations table `X` and target vector `y` are separate. We can accomplish that by using `unpack` from `MLJ`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────────────────┬────────────┬────────────┐\n",
      "│\u001b[1m Gender                            \u001b[0m│\u001b[1m Height     \u001b[0m│\u001b[1m Weight     \u001b[0m│\n",
      "│\u001b[90m CategoricalValue{String7, UInt32} \u001b[0m│\u001b[90m Float64    \u001b[0m│\u001b[90m Float64    \u001b[0m│\n",
      "│\u001b[90m Multiclass{2}                     \u001b[0m│\u001b[90m Continuous \u001b[0m│\u001b[90m Continuous \u001b[0m│\n",
      "├───────────────────────────────────┼────────────┼────────────┤\n",
      "│ Female                            │ 173.0      │ 82.0       │\n",
      "│ Female                            │ 187.0      │ 121.0      │\n",
      "│ Male                              │ 144.0      │ 145.0      │\n",
      "│ Male                              │ 156.0      │ 74.0       │\n",
      "│ Male                              │ 167.0      │ 151.0      │\n",
      "└───────────────────────────────────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "y, X = unpack(df, ==(:Index); rng=123);\n",
    "first(X, 5) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Splitting the data into train and test portions is also easy using `MLJ`'s `partition` function. `stratify=y` guarantees that the data is distributed in the same proportions as the original dataset in both splits which is more representative of the real world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((\u001b[1m399×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Gender \u001b[0m\u001b[1m Height  \u001b[0m\u001b[1m Weight  \u001b[0m\n",
       "     │\u001b[90m Cat…   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────\n",
       "   1 │ Female    179.0    150.0\n",
       "   2 │ Male      141.0     80.0\n",
       "   3 │ Male      179.0    152.0\n",
       "   4 │ Male      187.0    138.0\n",
       "   5 │ Male      148.0    155.0\n",
       "   6 │ Female    192.0    101.0\n",
       "   7 │ Male      145.0     78.0\n",
       "   8 │ Female    162.0    159.0\n",
       "  ⋮  │   ⋮        ⋮        ⋮\n",
       " 393 │ Female    161.0    154.0\n",
       " 394 │ Female    172.0    109.0\n",
       " 395 │ Female    163.0    159.0\n",
       " 396 │ Female    186.0    146.0\n",
       " 397 │ Male      194.0    106.0\n",
       " 398 │ Female    167.0    153.0\n",
       " 399 │ Female    162.0     64.0\n",
       "\u001b[36m                384 rows omitted\u001b[0m, \u001b[1m101×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Gender \u001b[0m\u001b[1m Height  \u001b[0m\u001b[1m Weight  \u001b[0m\n",
       "     │\u001b[90m Cat…   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────\n",
       "   1 │ Female    157.0     56.0\n",
       "   2 │ Male      180.0     75.0\n",
       "   3 │ Female    157.0    110.0\n",
       "   4 │ Female    182.0    143.0\n",
       "   5 │ Male      165.0    104.0\n",
       "   6 │ Male      182.0     73.0\n",
       "   7 │ Male      165.0     68.0\n",
       "   8 │ Male      166.0    107.0\n",
       "  ⋮  │   ⋮        ⋮        ⋮\n",
       "  95 │ Male      163.0    137.0\n",
       "  96 │ Female    188.0     99.0\n",
       "  97 │ Female    146.0    123.0\n",
       "  98 │ Male      186.0     68.0\n",
       "  99 │ Female    140.0     76.0\n",
       " 100 │ Female    168.0    139.0\n",
       " 101 │ Male      180.0    149.0\n",
       "\u001b[36m                 86 rows omitted\u001b[0m), (CategoricalArrays.CategoricalValue{Int64, UInt32}[5, 5, 5, 4, 5, 3, 4, 5, 5, 5  …  5, 4, 4, 5, 4, 5, 5, 3, 5, 2], CategoricalArrays.CategoricalValue{Int64, UInt32}[2, 2, 5, 5, 4, 2, 2, 4, 3, 3  …  2, 0, 0, 5, 3, 5, 2, 4, 5, 5]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, X_test), (y_train, y_test) = partition(\n",
    "\t(X, y),\n",
    "\t0.8,\n",
    "\tmulti = true,\n",
    "\tshuffle = true,\n",
    "\tstratify = y,\n",
    "\trng = Random.Xoshiro(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Always split the data before oversampling. If your test data has oversampled observations then train-test contamination has occurred; novel observations will not come from the oversampling function.\n",
    "\n",
    "## Oversampling\n",
    "\n",
    "\n",
    "\n",
    "Before deciding to oversample, let's see how adverse is the imbalance problem, if it exists. Ideally, you may as well check if the classification model is robust to this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ▇▇▇ 13 (6.6%) \n",
      "1: ▇▇▇▇▇▇ 22 (11.1%) \n",
      "3: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 68 (34.3%) \n",
      "2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 69 (34.8%) \n",
      "4: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 130 (65.7%) \n",
      "5: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 198 (100.0%) \n"
     ]
    }
   ],
   "source": [
    "checkbalance(y)             # comes from Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a class imbalance problem. Let's set the desired ratios so that the first two classes are 30%\n",
    "    of the majority class, the second two are 50% of the majority class and the rest as is (ignore in the dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64, Float64} with 4 entries:\n",
       "  0 => 0.3\n",
       "  2 => 0.5\n",
       "  3 => 0.5\n",
       "  1 => 0.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratios = Dict(0=>0.3, 1=>0.3, 2=>0.5, 3=>0.5)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use random oversampling to oversample the data. This particular model does not care about the scientific types of the data. It takes `X` and `y` as positional arguments and `ratios` and `rng` are the main keyword arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\u001b[1m514×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Gender \u001b[0m\u001b[1m Height  \u001b[0m\u001b[1m Weight  \u001b[0m\n",
       "     │\u001b[90m Cat…   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────\n",
       "   1 │ Female    179.0    150.0\n",
       "   2 │ Male      141.0     80.0\n",
       "   3 │ Male      179.0    152.0\n",
       "   4 │ Male      187.0    138.0\n",
       "   5 │ Male      148.0    155.0\n",
       "   6 │ Female    192.0    101.0\n",
       "   7 │ Male      145.0     78.0\n",
       "   8 │ Female    162.0    159.0\n",
       "  ⋮  │   ⋮        ⋮        ⋮\n",
       " 508 │ Female    196.0     50.0\n",
       " 509 │ Male      193.0     54.0\n",
       " 510 │ Male      182.0     50.0\n",
       " 511 │ Male      190.0     50.0\n",
       " 512 │ Male      190.0     50.0\n",
       " 513 │ Male      198.0     50.0\n",
       " 514 │ Male      198.0     50.0\n",
       "\u001b[36m                499 rows omitted\u001b[0m, CategoricalArrays.CategoricalValue{Int64, UInt32}[5, 5, 5, 4, 5, 3, 4, 5, 5, 5  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xover, yover = random_oversample(X_train, y_train; ratios, rng=42)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 47 (29.7%) \n",
      "1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 47 (29.7%) \n",
      "2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 79 (50.0%) \n",
      "3: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 79 (50.0%) \n",
      "4: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 104 (65.8%) \n",
      "5: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 158 (100.0%) \n"
     ]
    }
   ],
   "source": [
    "checkbalance(yover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indeeds aligns with the desired ratios we have set earlier.\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "\n",
    "\n",
    "Because we have scientific types setup, we can easily check what models will be able to train on our data. This should guarantee that the model we choose won't throw an error due to types after feeding it the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = CatBoostClassifier, package_name = CatBoost, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models(matching(Xover, yover))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go for a decision tree form BetaML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"BetaML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(DecisionTreeClassifier(max_depth = 5, …), …).\n",
      "└ @ MLJBase /Users/essam/.julia/packages/MLJBase/ByFwA/src/machines.jl:492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: DecisionTreeClassifier(max_depth = 5, …)\n",
       "  args: \n",
       "    1:\tSource @027 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}\n",
       "    2:\tSource @092 ⏎ AbstractVector{OrderedFactor{6}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load the model\n",
    "DecisionTreeClassifier = @load DecisionTreeClassifier pkg=BetaML verbosity=0\n",
    "\n",
    "# 2. Instantiate it\n",
    "model = DecisionTreeClassifier(max_depth=5, rng=Random.Xoshiro(42))\n",
    "\n",
    "# 3. Wrap it with the data in a machine\n",
    "mach = machine(model, X_train, y_train)\n",
    "\n",
    "# 4. fit the machine learning model\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(DecisionTreeClassifier(max_depth = 5, …), …).\n",
      "└ @ MLJBase /Users/essam/.julia/packages/MLJBase/ByFwA/src/machines.jl:492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: DecisionTreeClassifier(max_depth = 5, …)\n",
       "  args: \n",
       "    1:\tSource @592 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}\n",
       "    2:\tSource @711 ⏎ AbstractVector{OrderedFactor{6}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Wrap it with the data in a machine\n",
    "mach_over = machine(model, Xover, yover)\n",
    "\n",
    "# 4. fit the machine learning model\n",
    "fit!(mach_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To evaluate the model, we will use the balanced accuracy metric which equally account for all classes. For instance, if we have two classes and we correctly classify 100% of the examples in the first and 50% of the examples in the second then the balanced accuracy is $(100+50)/2=75%$. This holds regardless to how big or small each class is.\n",
    "\n",
    "The `predict_mode` will return a vector of predictions given `X_test` and the fitted machine. It's different in that `predict` in not returning probablities the model assigns to each class; instead, it returns the classes with the maximum probabilities; i.e., the modes.\n",
    "\n",
    "### Before Oversampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = predict_mode(mach, X_test)                         \n",
    "\n",
    "score = round(balanced_accuracy(y_pred, y_test), digits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_over = predict_mode(mach_over, X_test)\n",
    "score = round(balanced_accuracy(y_pred_over, y_test), digits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model - Revisited\n",
    "\n",
    "We have previously evaluated the model using a single point estimate of the balanced accuracy resulting in a `13%` improvement. A more precise evaluation would use cross validation to combine many different point estimates into a more precise one (their average). The standard deviation among such point estimates also allows us to quantify the uncertainty of the estimate; a smaller standard deviation would imply a smaller confidence interval at the same probability.\n",
    "\n",
    "### Before Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 folds:  60%[===============>         ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 folds: 100%[=========================] Time: 0:00:00\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  model, measure, operation, measurement, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_rows, resampling, repeats\n",
       "Extract:\n",
       "┌─────────────────────┬──────────────┬─────────────┬─────────┬──────────────────\n",
       "│\u001b[22m measure             \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m 1.96*SE \u001b[0m│\u001b[22m per_fold       \u001b[0m ⋯\n",
       "├─────────────────────┼──────────────┼─────────────┼─────────┼──────────────────\n",
       "│ BalancedAccuracy(   │ predict_mode │ 0.621       │ 0.0913  │ [0.593, 0.473,  ⋯\n",
       "│   adjusted = false) │              │             │         │                 ⋯\n",
       "└─────────────────────┴──────────────┴─────────────┴─────────┴──────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv=CV(nfolds=10)\n",
    "evaluate!(mach, resampling=cv, measure=balanced_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the normality assumption, the `95%` confidence interval is `62.1±9.13%` which is pretty big. Let's see how it looks after oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this seems really nontrivial since resampling will have to be performed before training the model on each fold during cross-validation. Thankfully, the `MLJBalancing` helps us avoid doing this manually by offering `BalancedModel` where we can wrap any `MLJ` classification model with an arbitrary number of `Imbalance.jl` resamplers in a pipeline that behaves like a single `MLJ` model.\n",
    "\n",
    "In this, we must construct the resampling model via it's `MLJ` interface then pass it along with the classification model to `BalancedModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trained Machine; does not cache data\n",
       "  model: BalancedModelProbabilistic(model = DecisionTreeClassifier(max_depth = 5, …), …)\n",
       "  args: \n",
       "    1:\tSource @099 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}\n",
       "    2:\tSource @071 ⏎ AbstractVector{OrderedFactor{6}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Instantiate the models\n",
    "oversampler = Imbalance.MLJ.RandomOversampler(ratios=ratios, rng=42)\n",
    "model = DecisionTreeClassifier(max_depth=5, rng=Random.Xoshiro(42))\n",
    "\n",
    "# 2.1 Wrap them in one model\n",
    "balanced_model = BalancedModel(model=model, balancer1=oversampler)\n",
    "\n",
    "# 3. Wrap it with the data in a machine\n",
    "mach_over = machine(balanced_model, X_train, y_train, scitype_check_level=0)\n",
    "\n",
    "# 4. fit the machine learning model\n",
    "fit!(mach_over, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily confirm that this is equivalent to what we had earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_mode(mach_over, X_test) ==  y_pred_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 folds:  40%[==========>              ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 folds:  80%[====================>    ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 folds: 100%[=========================] Time: 0:00:00\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  model, measure, operation, measurement, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_rows, resampling, repeats\n",
       "Extract:\n",
       "┌─────────────────────┬──────────────┬─────────────┬─────────┬──────────────────\n",
       "│\u001b[22m measure             \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m 1.96*SE \u001b[0m│\u001b[22m per_fold       \u001b[0m ⋯\n",
       "├─────────────────────┼──────────────┼─────────────┼─────────┼──────────────────\n",
       "│ BalancedAccuracy(   │ predict_mode │ 0.7         │ 0.0717  │ [0.7, 0.536, 0. ⋯\n",
       "│   adjusted = false) │              │             │         │                 ⋯\n",
       "└─────────────────────┴──────────────┴─────────────┴─────────┴──────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv=CV(nfolds=10)\n",
    "evaluate!(mach_over, resampling=cv, measure=balanced_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in an interval `70±7.2%` which can be viewed as a reasonable improvement over `62.1±9.13%`. The uncertainty in the intervals can be explained by the fact that the dataset is small with many classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook walkthrough.ipynb to markdown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 19604 bytes to walkthrough.md\n"
     ]
    }
   ],
   "source": [
    "from convert import convert_to_md; convert_to_md('walkthrough', copy=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
